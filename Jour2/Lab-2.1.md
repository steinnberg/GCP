# Lab 2.1 â€” Automatiser le chargement dâ€™un CSV dans BigQuery via Cloud Function + Scheduler

## ğŸŒŸ Objectif
DÃ©ployer une **Cloud Function** qui :
- Se connecte Ã  un fichier `.csv` dans un bucket GCS
- Le charge dans une **table BigQuery**
- Est planifiÃ©e automatiquement via **Cloud Scheduler** (ex : chaque jour Ã  9h)

---

## ğŸ“š PrÃ©requis
- Projet GCP activÃ© (`my-project-id`)
- Fichier CSV dans un bucket GCS (`gs://my-bucket/olist_sample.csv`)
- Table BigQuery cible (`dataset.olist_table`)
- API activÃ©es :
  - Cloud Functions
  - Cloud Scheduler
  - BigQuery
  - Cloud Storage

---

## âœï¸ Etape 1 : CrÃ©er le bucket et uploader le CSV
```bash
# CrÃ©ation du bucket
gsutil mb -l europe-west1 gs://my-bucket

# Upload du fichier CSV
gsutil cp olist_sample.csv gs://my-bucket/
```

---

## ğŸ§  Etape 2 : CrÃ©er la Cloud Function

### Structure du dossier
```
cloud_function/
â”œâ”€â”€ main.py
â””â”€â”€ requirements.txt
```

### Contenu de `main.py`
```python
import functions_framework
from google.cloud import bigquery

@functions_framework.http
def upload_csv_to_bq(request):
    client = bigquery.Client()

    uri = "gs://my-bucket/olist_sample.csv"
    table_id = "my-project-id.dataset.olist_table"

    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,
        autodetect=True,
        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
    )

    load_job = client.load_table_from_uri(
        uri, table_id, job_config=job_config
    )

    load_job.result()

    return f"âœ… CSV uploaded to {table_id}."
```

### Contenu de `requirements.txt`
```
functions-framework==3.5.0
google-cloud-bigquery==3.14.1
```

---

## ğŸš€ Etape 3 : DÃ©ploiement de la Cloud Function
```bash
gcloud functions deploy upload_csv_to_bq \
  --runtime python310 \
  --trigger-http \
  --allow-unauthenticated \
  --entry-point upload_csv_to_bq \
  --source=./cloud_function \
  --region=europe-west1
```

---

## â° Etape 4 : CrÃ©ation du Cloud Scheduler
```bash
gcloud scheduler jobs create http call-upload-csv-job \
  --schedule="0 9 * * *" \
  --uri="https://REGION-PROJECT_ID.cloudfunctions.net/upload_csv_to_bq" \
  --http-method=GET \
  --time-zone="Europe/Paris"
```
Remplace `REGION-PROJECT_ID` avec lâ€™URL rÃ©elle de ta Cloud Function.

---

## ğŸ“ Etape 5 : Test Final
- VÃ©rifier le fichier dans `gs://my-bucket/olist_sample.csv`
- Lancer le Scheduler manuellement via l'interface GCP
- ContrÃ´ler les donnÃ©es dans BigQuery : `dataset.olist_table`

---

## ğŸ”¹ RÃ©sultat attendu
Une pipeline entiÃ¨rement automatisÃ©e : CSV â” GCS â” Cloud Function â” BigQuery, exÃ©cutÃ©e chaque jour par Cloud Scheduler.
